{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                            f1_score, roc_auc_score)\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Loading data\n",
    "df_model = pd.read_excel(\n",
    "    Path(r\"data_source\\xxx.xlsx\")\n",
    ")\n",
    "df_annotated = pd.read_excel(\n",
    "    Path(r\"data_source\\xxx.xlsx\")\n",
    ")\n",
    "\n",
    "# rename\n",
    "df_model = df_model.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"Practical barriers to vaccination (-)\",\n",
    "        \"Perceived barriers to accepting vaccines (-)\",\n",
    "        \"Perceived benefits (+)\",\n",
    "        \"Misinformation (-)\",\n",
    "        \"Perceived Disease Risk (+)\",\n",
    "        \"Social norms  cues to action (+)\",\n",
    "        \"Attitude\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "df_annotated = df_annotated.loc[\n",
    "    :,\n",
    "    [\n",
    "        \"Practical barriers to vaccination (-)\",\n",
    "        \"Perceived barriers to accepting vaccines (-)\",\n",
    "        \"Perceived benefits (+)\",\n",
    "        \"Misinformation (-)\",\n",
    "        \"Perceived Disease Risk (+)\",\n",
    "        \"Social norms  cues to action (+)\",\n",
    "        \"Attitude\",\n",
    "    ],\n",
    "]\n",
    "\n",
    "assert len(df_model) == len(df_annotated), \"DataFrames have different lengths\"\n",
    "\n",
    "binary_categories = [   \n",
    "                        \"Practical barriers to vaccination (-)\",        \n",
    "                        \"Perceived barriers to accepting vaccines (-)\",        \n",
    "                        \"Behavior\",        \n",
    "                        \"Perceived benefits (+)\",        \n",
    "                        \"Misinformation (-)\",\n",
    "                        \"Perceived Disease Risk (+)\",        \n",
    "                        \"Social norms  cues to action (+)\",  \n",
    "                    ]\n",
    "\n",
    "results = {\n",
    "    'Category': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'AUC': [],\n",
    "    'Model Pos/Neg Count': [],\n",
    "    'Annotated Pos/Neg Count': []\n",
    "}\n",
    "\n",
    "attitude_results = {\n",
    "    'Class': [],\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1': [],\n",
    "    'AUC (OvR)': [],\n",
    "    'Model Count': [],\n",
    "    'Annotated Count': []\n",
    "}\n",
    "\n",
    "attitude_macro_results = {\n",
    "    'Metric': [],\n",
    "    'Value': [],\n",
    "    'Model Class Distribution': [],\n",
    "    'Annotated Class Distribution': []\n",
    "}\n",
    "\n",
    "# Bootstrap parameter\n",
    "n_bootstraps = 10000\n",
    "alpha = 0.05  # 95% CI\n",
    "\n",
    "def format_result(point, lower, upper):\n",
    "\n",
    "    return f\"{point:.4f} ({lower:.4f}, {upper:.4f})\"\n",
    "\n",
    "def format_counts(count_dict):\n",
    "\n",
    "    return \"\\n\".join([f\"{k}: {v}\" for k, v in count_dict.items()])\n",
    "\n",
    "def calculate_ci(data):\n",
    "\n",
    "    lower = np.percentile(data, 100 * alpha / 2)\n",
    "    upper = np.percentile(data, 100 * (1 - alpha / 2))\n",
    "    return lower, upper\n",
    "\n",
    "# binary category estimation\n",
    "for category in binary_categories:\n",
    "    print(category)\n",
    "    print(f\"\\nEvaluating binary category: {category}\")\n",
    "    \n",
    "    y_true = df_annotated[category].values\n",
    "    y_pred = df_model[category].values\n",
    "    \n",
    "    y_true = (y_true > 0).astype(int)\n",
    "    y_pred = (y_pred > 0).astype(int)\n",
    "    \n",
    "    # class counting\n",
    "    model_pos = sum(y_pred)\n",
    "    model_neg = len(y_pred) - model_pos\n",
    "    annotated_pos = sum(y_true)\n",
    "    annotated_neg = len(y_true) - annotated_pos\n",
    "    \n",
    "    model_counts = {\"Model Pos\": model_pos, \"Model Neg\": model_neg}\n",
    "    annotated_counts = {\"Annotated Pos\": annotated_pos, \"Annotated Neg\": annotated_neg}\n",
    "    \n",
    "    # initialize bootstrap storage\n",
    "    boot_acc, boot_prec, boot_rec, boot_f1, boot_auc = [], [], [], [], []\n",
    "    \n",
    "    # bootstrap sampling\n",
    "    for _ in tqdm(range(n_bootstraps), desc=f\"Bootstrapping {category}\"):\n",
    "        indices = np.random.choice(len(y_true), len(y_true), replace=True)\n",
    "        sample_true = y_true[indices]\n",
    "        sample_pred = y_pred[indices]\n",
    "        \n",
    "        try:\n",
    "            acc = accuracy_score(sample_true, sample_pred)\n",
    "            prec = precision_score(sample_true, sample_pred, zero_division=0)\n",
    "            rec = recall_score(sample_true, sample_pred, zero_division=0)\n",
    "            f1 = f1_score(sample_true, sample_pred, zero_division=0)\n",
    "            \n",
    "            auc = roc_auc_score(sample_true, sample_pred)\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "        boot_acc.append(acc)\n",
    "        boot_prec.append(prec)\n",
    "        boot_rec.append(rec)\n",
    "        boot_f1.append(f1)\n",
    "        boot_auc.append(auc)\n",
    "    \n",
    "    # point estimation\n",
    "    point_acc = accuracy_score(y_true, y_pred)\n",
    "    point_prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    point_rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    point_f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    \n",
    "    try:\n",
    "        point_auc = roc_auc_score(y_true, y_pred)\n",
    "    except:\n",
    "        point_auc = np.nan\n",
    "    \n",
    "    # CI estimation\n",
    "    acc_lower, acc_upper = calculate_ci(boot_acc)\n",
    "    prec_lower, prec_upper = calculate_ci(boot_prec)\n",
    "    rec_lower, rec_upper = calculate_ci(boot_rec)\n",
    "    f1_lower, f1_upper = calculate_ci(boot_f1)\n",
    "    auc_lower, auc_upper = calculate_ci(boot_auc) if boot_auc else (np.nan, np.nan)\n",
    "    \n",
    "    # save results\n",
    "    results['Category'].append(category)\n",
    "    results['Accuracy'].append(format_result(point_acc, acc_lower, acc_upper))\n",
    "    results['Precision'].append(format_result(point_prec, prec_lower, prec_upper))\n",
    "    results['Recall'].append(format_result(point_rec, rec_lower, rec_upper))\n",
    "    results['F1'].append(format_result(point_f1, f1_lower, f1_upper))\n",
    "    results['AUC'].append(format_result(point_auc, auc_lower, auc_upper) if not np.isnan(point_auc) else \"N/A\")\n",
    "    results['Model Pos/Neg Count'].append(format_counts(model_counts))\n",
    "    results['Annotated Pos/Neg Count'].append(format_counts(annotated_counts))\n",
    "\n",
    "# 3 category evaluation\n",
    "print(\"\\nEvaluating multiclass category: Attitude\")\n",
    "y_true_att = df_annotated[\"Attitude\"].values\n",
    "y_pred_att = df_model[\"Attitude\"].values\n",
    "\n",
    "y_true_att = y_true_att.astype(int)\n",
    "y_pred_att = y_pred_att.astype(int)\n",
    "\n",
    "# class counting\n",
    "model_class_counts = {f\"Model Class {i}\": sum(y_pred_att == i) for i in range(3)}\n",
    "annotated_class_counts = {f\"Annotated Class {i}\": sum(y_true_att == i) for i in range(3)}\n",
    "\n",
    "# initializebootstrap storage\n",
    "boot_acc_att, boot_prec_macro_att, boot_rec_macro_att, boot_f1_macro_att = [], [], [], []\n",
    "boot_prec_per_class, boot_rec_per_class, boot_f1_per_class = [], [], []\n",
    "boot_auc_macro_att, boot_auc_per_class = [], []\n",
    "\n",
    "y_true_bin = label_binarize(y_true_att, classes=[0, 1, 2])\n",
    "\n",
    "# bootstrap sampling\n",
    "for _ in tqdm(range(n_bootstraps), desc=\"Bootstrapping Attitude\"):\n",
    "    indices = np.random.choice(len(y_true_att), len(y_true_att), replace=True)\n",
    "    sample_true = y_true_att[indices]\n",
    "    sample_pred = y_pred_att[indices]\n",
    "    sample_true_bin = y_true_bin[indices]\n",
    "    \n",
    "    try:\n",
    "        acc = accuracy_score(sample_true, sample_pred)\n",
    "        \n",
    "        prec_macro = precision_score(sample_true, sample_pred, average='macro', zero_division=0)\n",
    "        rec_macro = recall_score(sample_true, sample_pred, average='macro', zero_division=0)\n",
    "        f1_macro = f1_score(sample_true, sample_pred, average='macro', zero_division=0)\n",
    "        \n",
    "        prec_per_class = precision_score(sample_true, sample_pred, average=None, zero_division=0)\n",
    "        rec_per_class = recall_score(sample_true, sample_pred, average=None, zero_division=0)\n",
    "        f1_per_class = f1_score(sample_true, sample_pred, average=None, zero_division=0)\n",
    "        \n",
    "\n",
    "        auc_scores = []\n",
    "        for i in range(3):\n",
    "            try:\n",
    "                auc = roc_auc_score(sample_true_bin[:, i], (sample_pred == i).astype(int))\n",
    "                auc_scores.append(auc)\n",
    "            except:\n",
    "                auc_scores.append(np.nan)\n",
    "        \n",
    "        auc_macro = np.nanmean(auc_scores) if not all(np.isnan(auc_scores)) else np.nan\n",
    "    except Exception as e:\n",
    "        print(f\"Error in bootstrap: {e}\")\n",
    "        continue\n",
    "    \n",
    "    boot_acc_att.append(acc)\n",
    "    boot_prec_macro_att.append(prec_macro)\n",
    "    boot_rec_macro_att.append(rec_macro)\n",
    "    boot_f1_macro_att.append(f1_macro)\n",
    "    boot_prec_per_class.append(prec_per_class)\n",
    "    boot_rec_per_class.append(rec_per_class)\n",
    "    boot_f1_per_class.append(f1_per_class)\n",
    "    boot_auc_macro_att.append(auc_macro)\n",
    "    boot_auc_per_class.append(auc_scores)\n",
    "\n",
    "# point estimation\n",
    "point_acc_att = accuracy_score(y_true_att, y_pred_att)\n",
    "point_prec_macro_att = precision_score(y_true_att, y_pred_att, average='macro', zero_division=0)\n",
    "point_rec_macro_att = recall_score(y_true_att, y_pred_att, average='macro', zero_division=0)\n",
    "point_f1_macro_att = f1_score(y_true_att, y_pred_att, average='macro', zero_division=0)\n",
    "point_prec_per_class = precision_score(y_true_att, y_pred_att, average=None, zero_division=0)\n",
    "point_rec_per_class = recall_score(y_true_att, y_pred_att, average=None, zero_division=0)\n",
    "point_f1_per_class = f1_score(y_true_att, y_pred_att, average=None, zero_division=0)\n",
    "\n",
    "auc_scores = []\n",
    "for i in range(3):\n",
    "    try:\n",
    "        auc = roc_auc_score(y_true_bin[:, i], (y_pred_att == i).astype(int))\n",
    "        auc_scores.append(auc)\n",
    "    except:\n",
    "        auc_scores.append(np.nan)\n",
    "point_auc_macro_att = np.nanmean(auc_scores) if not all(np.isnan(auc_scores)) else np.nan\n",
    "\n",
    "# CI estimation\n",
    "acc_lower_att, acc_upper_att = calculate_ci(boot_acc_att)\n",
    "prec_macro_lower, prec_macro_upper = calculate_ci(boot_prec_macro_att)\n",
    "rec_macro_lower, rec_macro_upper = calculate_ci(boot_rec_macro_att)\n",
    "f1_macro_lower, f1_macro_upper = calculate_ci(boot_f1_macro_att)\n",
    "auc_macro_lower, auc_macro_upper = calculate_ci(boot_auc_macro_att) if boot_auc_macro_att else (np.nan, np.nan)\n",
    "\n",
    "# save results marco\n",
    "attitude_macro_results['Metric'].append('Accuracy')\n",
    "attitude_macro_results['Value'].append(format_result(point_acc_att, acc_lower_att, acc_upper_att))\n",
    "attitude_macro_results['Model Class Distribution'].append(format_counts(model_class_counts))\n",
    "attitude_macro_results['Annotated Class Distribution'].append(format_counts(annotated_class_counts))\n",
    "\n",
    "attitude_macro_results['Metric'].append('Precision (macro)')\n",
    "attitude_macro_results['Value'].append(format_result(point_prec_macro_att, prec_macro_lower, prec_macro_upper))\n",
    "attitude_macro_results['Model Class Distribution'].append(\"\")\n",
    "attitude_macro_results['Annotated Class Distribution'].append(\"\")\n",
    "\n",
    "attitude_macro_results['Metric'].append('Recall (macro)')\n",
    "attitude_macro_results['Value'].append(format_result(point_rec_macro_att, rec_macro_lower, rec_macro_upper))\n",
    "attitude_macro_results['Model Class Distribution'].append(\"\")\n",
    "attitude_macro_results['Annotated Class Distribution'].append(\"\")\n",
    "\n",
    "attitude_macro_results['Metric'].append('F1 (macro)')\n",
    "attitude_macro_results['Value'].append(format_result(point_f1_macro_att, f1_macro_lower, f1_macro_upper))\n",
    "attitude_macro_results['Model Class Distribution'].append(\"\")\n",
    "attitude_macro_results['Annotated Class Distribution'].append(\"\")\n",
    "\n",
    "attitude_macro_results['Metric'].append('AUC (macro)')\n",
    "attitude_macro_results['Value'].append(format_result(point_auc_macro_att, auc_macro_lower, auc_macro_upper) if not np.isnan(point_auc_macro_att) else \"N/A\")\n",
    "attitude_macro_results['Model Class Distribution'].append(\"\")\n",
    "attitude_macro_results['Annotated Class Distribution'].append(\"\")\n",
    "\n",
    "# calculation\n",
    "for class_idx in range(3):\n",
    "    # point estimation\n",
    "    class_prec = [prec[class_idx] for prec in boot_prec_per_class if len(prec) > class_idx]\n",
    "    class_rec = [rec[class_idx] for rec in boot_rec_per_class if len(rec) > class_idx]\n",
    "    class_f1 = [f1[class_idx] for f1 in boot_f1_per_class if len(f1) > class_idx]\n",
    "    class_auc = [auc[class_idx] for auc in boot_auc_per_class if len(auc) > class_idx and not np.isnan(auc[class_idx])]\n",
    "    \n",
    "    # CI estimation\n",
    "    prec_lower, prec_upper = calculate_ci(class_prec)\n",
    "    rec_lower, rec_upper = calculate_ci(class_rec)\n",
    "    f1_lower, f1_upper = calculate_ci(class_f1)\n",
    "    auc_lower, auc_upper = calculate_ci(class_auc) if class_auc else (np.nan, np.nan)\n",
    "    \n",
    "    # class conuting\n",
    "    model_count = model_class_counts[f\"Model Class {class_idx}\"]\n",
    "    annotated_count = annotated_class_counts[f\"Annotated Class {class_idx}\"]\n",
    "    \n",
    "    # save results\n",
    "    attitude_results['Class'].append(f\"Class {class_idx}\")\n",
    "    attitude_results['Accuracy'].append(format_result(point_acc_att, acc_lower_att, acc_upper_att))\n",
    "    attitude_results['Precision'].append(format_result(point_prec_per_class[class_idx], prec_lower, prec_upper))\n",
    "    attitude_results['Recall'].append(format_result(point_rec_per_class[class_idx], rec_lower, rec_upper))\n",
    "    attitude_results['F1'].append(format_result(point_f1_per_class[class_idx], f1_lower, f1_upper))\n",
    "    attitude_results['AUC (OvR)'].append(format_result(auc_scores[class_idx], auc_lower, auc_upper) if not np.isnan(auc_scores[class_idx]) else \"N/A\")\n",
    "    attitude_results['Model Count'].append(f\"Model: {model_count}\")\n",
    "    attitude_results['Annotated Count'].append(f\"Annotated: {annotated_count}\")\n",
    "\n",
    "# to DataFrame\n",
    "binary_results_df = pd.DataFrame(results)\n",
    "attitude_results_df = pd.DataFrame(attitude_results)\n",
    "attitude_macro_df = pd.DataFrame(attitude_macro_results)\n",
    "\n",
    "print(\"\\nBinary Categories Results:\")\n",
    "print(binary_results_df)\n",
    "print(\"\\nAttitude (Multiclass) Results - Macro Average:\")\n",
    "print(attitude_macro_df)\n",
    "print(\"\\nAttitude (Multiclass) Results - Per Class:\")\n",
    "print(attitude_results_df)\n",
    "\n",
    "# save to Excel\n",
    "output_path = Path(r\"data_source\\xxx.xlsx\")\n",
    "with pd.ExcelWriter(output_path) as writer:\n",
    "    binary_results_df.to_excel(writer, sheet_name=\"Binary Categories\", index=False)\n",
    "    attitude_macro_df.to_excel(writer, sheet_name=\"Attitude (Macro)\", index=False)\n",
    "    attitude_results_df.to_excel(writer, sheet_name=\"Attitude (Per Class)\", index=False)\n",
    "print(f\"\\nResults saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
